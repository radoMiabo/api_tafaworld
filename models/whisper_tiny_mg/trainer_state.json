{
  "best_metric": 54.05011655011654,
  "best_model_checkpoint": "./whisper-tiny-mg/checkpoint-4000",
  "epoch": 35.39823008849557,
  "eval_steps": 1000,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.22123893805309736,
      "grad_norm": 39.606651306152344,
      "learning_rate": 4.4e-07,
      "loss": 5.5243,
      "step": 25
    },
    {
      "epoch": 0.4424778761061947,
      "grad_norm": 30.932270050048828,
      "learning_rate": 9.400000000000001e-07,
      "loss": 5.1701,
      "step": 50
    },
    {
      "epoch": 0.6637168141592921,
      "grad_norm": 26.209129333496094,
      "learning_rate": 1.44e-06,
      "loss": 4.5288,
      "step": 75
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 23.263187408447266,
      "learning_rate": 1.94e-06,
      "loss": 3.7256,
      "step": 100
    },
    {
      "epoch": 1.1061946902654867,
      "grad_norm": 21.56170082092285,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 3.0399,
      "step": 125
    },
    {
      "epoch": 1.3274336283185841,
      "grad_norm": 18.772991180419922,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 2.5063,
      "step": 150
    },
    {
      "epoch": 1.5486725663716814,
      "grad_norm": 17.104291915893555,
      "learning_rate": 3.44e-06,
      "loss": 2.1126,
      "step": 175
    },
    {
      "epoch": 1.7699115044247788,
      "grad_norm": 15.392271041870117,
      "learning_rate": 3.94e-06,
      "loss": 1.8936,
      "step": 200
    },
    {
      "epoch": 1.991150442477876,
      "grad_norm": 14.62234115600586,
      "learning_rate": 4.440000000000001e-06,
      "loss": 1.7105,
      "step": 225
    },
    {
      "epoch": 2.2123893805309733,
      "grad_norm": 15.269026756286621,
      "learning_rate": 4.94e-06,
      "loss": 1.4671,
      "step": 250
    },
    {
      "epoch": 2.433628318584071,
      "grad_norm": 13.63053035736084,
      "learning_rate": 5.4400000000000004e-06,
      "loss": 1.3705,
      "step": 275
    },
    {
      "epoch": 2.6548672566371683,
      "grad_norm": 14.559429168701172,
      "learning_rate": 5.94e-06,
      "loss": 1.225,
      "step": 300
    },
    {
      "epoch": 2.8761061946902657,
      "grad_norm": 13.981684684753418,
      "learning_rate": 6.440000000000001e-06,
      "loss": 1.1655,
      "step": 325
    },
    {
      "epoch": 3.0973451327433628,
      "grad_norm": 12.68132495880127,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 1.0757,
      "step": 350
    },
    {
      "epoch": 3.3185840707964602,
      "grad_norm": 13.158202171325684,
      "learning_rate": 7.440000000000001e-06,
      "loss": 0.9504,
      "step": 375
    },
    {
      "epoch": 3.5398230088495577,
      "grad_norm": 12.62331771850586,
      "learning_rate": 7.94e-06,
      "loss": 0.9428,
      "step": 400
    },
    {
      "epoch": 3.7610619469026547,
      "grad_norm": 11.039656639099121,
      "learning_rate": 8.44e-06,
      "loss": 0.855,
      "step": 425
    },
    {
      "epoch": 3.982300884955752,
      "grad_norm": 11.42246150970459,
      "learning_rate": 8.94e-06,
      "loss": 0.8259,
      "step": 450
    },
    {
      "epoch": 4.20353982300885,
      "grad_norm": 11.89397144317627,
      "learning_rate": 9.440000000000001e-06,
      "loss": 0.7061,
      "step": 475
    },
    {
      "epoch": 4.424778761061947,
      "grad_norm": 13.986906051635742,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.6923,
      "step": 500
    },
    {
      "epoch": 4.646017699115045,
      "grad_norm": 10.595550537109375,
      "learning_rate": 9.951111111111111e-06,
      "loss": 0.6509,
      "step": 525
    },
    {
      "epoch": 4.867256637168142,
      "grad_norm": 10.666378021240234,
      "learning_rate": 9.895555555555557e-06,
      "loss": 0.622,
      "step": 550
    },
    {
      "epoch": 5.088495575221239,
      "grad_norm": 8.690300941467285,
      "learning_rate": 9.84e-06,
      "loss": 0.5947,
      "step": 575
    },
    {
      "epoch": 5.3097345132743365,
      "grad_norm": 11.0014009475708,
      "learning_rate": 9.784444444444445e-06,
      "loss": 0.5013,
      "step": 600
    },
    {
      "epoch": 5.530973451327434,
      "grad_norm": 9.066227912902832,
      "learning_rate": 9.72888888888889e-06,
      "loss": 0.4883,
      "step": 625
    },
    {
      "epoch": 5.752212389380531,
      "grad_norm": 8.934707641601562,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.4887,
      "step": 650
    },
    {
      "epoch": 5.9734513274336285,
      "grad_norm": 10.483678817749023,
      "learning_rate": 9.617777777777778e-06,
      "loss": 0.4882,
      "step": 675
    },
    {
      "epoch": 6.1946902654867255,
      "grad_norm": 8.311570167541504,
      "learning_rate": 9.562222222222223e-06,
      "loss": 0.3772,
      "step": 700
    },
    {
      "epoch": 6.415929203539823,
      "grad_norm": 8.37535285949707,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.3731,
      "step": 725
    },
    {
      "epoch": 6.6371681415929205,
      "grad_norm": 9.782217025756836,
      "learning_rate": 9.451111111111112e-06,
      "loss": 0.3853,
      "step": 750
    },
    {
      "epoch": 6.8584070796460175,
      "grad_norm": 8.51614761352539,
      "learning_rate": 9.395555555555556e-06,
      "loss": 0.3672,
      "step": 775
    },
    {
      "epoch": 7.079646017699115,
      "grad_norm": 7.9660725593566895,
      "learning_rate": 9.340000000000002e-06,
      "loss": 0.3384,
      "step": 800
    },
    {
      "epoch": 7.300884955752212,
      "grad_norm": 8.107606887817383,
      "learning_rate": 9.284444444444444e-06,
      "loss": 0.2734,
      "step": 825
    },
    {
      "epoch": 7.522123893805309,
      "grad_norm": 7.206039905548096,
      "learning_rate": 9.22888888888889e-06,
      "loss": 0.2895,
      "step": 850
    },
    {
      "epoch": 7.743362831858407,
      "grad_norm": 7.410472869873047,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.2771,
      "step": 875
    },
    {
      "epoch": 7.964601769911504,
      "grad_norm": 9.175496101379395,
      "learning_rate": 9.117777777777778e-06,
      "loss": 0.2989,
      "step": 900
    },
    {
      "epoch": 8.185840707964601,
      "grad_norm": 6.2392425537109375,
      "learning_rate": 9.062222222222224e-06,
      "loss": 0.2072,
      "step": 925
    },
    {
      "epoch": 8.4070796460177,
      "grad_norm": 9.37993335723877,
      "learning_rate": 9.006666666666666e-06,
      "loss": 0.2137,
      "step": 950
    },
    {
      "epoch": 8.628318584070797,
      "grad_norm": 8.211576461791992,
      "learning_rate": 8.951111111111112e-06,
      "loss": 0.2157,
      "step": 975
    },
    {
      "epoch": 8.849557522123893,
      "grad_norm": 6.433940410614014,
      "learning_rate": 8.895555555555556e-06,
      "loss": 0.221,
      "step": 1000
    },
    {
      "epoch": 8.849557522123893,
      "eval_loss": 0.8127722144126892,
      "eval_runtime": 74.9066,
      "eval_samples_per_second": 3.538,
      "eval_steps_per_second": 0.454,
      "eval_wer": 55.59440559440559,
      "step": 1000
    },
    {
      "epoch": 9.070796460176991,
      "grad_norm": 5.333868503570557,
      "learning_rate": 8.84e-06,
      "loss": 0.1997,
      "step": 1025
    },
    {
      "epoch": 9.29203539823009,
      "grad_norm": 5.748682022094727,
      "learning_rate": 8.784444444444446e-06,
      "loss": 0.147,
      "step": 1050
    },
    {
      "epoch": 9.513274336283185,
      "grad_norm": 7.996050834655762,
      "learning_rate": 8.72888888888889e-06,
      "loss": 0.1688,
      "step": 1075
    },
    {
      "epoch": 9.734513274336283,
      "grad_norm": 7.158821105957031,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.1593,
      "step": 1100
    },
    {
      "epoch": 9.955752212389381,
      "grad_norm": 7.5886125564575195,
      "learning_rate": 8.617777777777778e-06,
      "loss": 0.1696,
      "step": 1125
    },
    {
      "epoch": 10.176991150442477,
      "grad_norm": 4.475862979888916,
      "learning_rate": 8.562222222222224e-06,
      "loss": 0.1193,
      "step": 1150
    },
    {
      "epoch": 10.398230088495575,
      "grad_norm": 5.674215793609619,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.1259,
      "step": 1175
    },
    {
      "epoch": 10.619469026548673,
      "grad_norm": 4.485940456390381,
      "learning_rate": 8.451111111111112e-06,
      "loss": 0.1164,
      "step": 1200
    },
    {
      "epoch": 10.84070796460177,
      "grad_norm": 4.620321273803711,
      "learning_rate": 8.395555555555557e-06,
      "loss": 0.1176,
      "step": 1225
    },
    {
      "epoch": 11.061946902654867,
      "grad_norm": 3.4070868492126465,
      "learning_rate": 8.34e-06,
      "loss": 0.1152,
      "step": 1250
    },
    {
      "epoch": 11.283185840707965,
      "grad_norm": 3.990659475326538,
      "learning_rate": 8.284444444444446e-06,
      "loss": 0.0731,
      "step": 1275
    },
    {
      "epoch": 11.504424778761061,
      "grad_norm": 3.802327871322632,
      "learning_rate": 8.22888888888889e-06,
      "loss": 0.0875,
      "step": 1300
    },
    {
      "epoch": 11.725663716814159,
      "grad_norm": 4.2819294929504395,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0845,
      "step": 1325
    },
    {
      "epoch": 11.946902654867257,
      "grad_norm": 4.288971900939941,
      "learning_rate": 8.11777777777778e-06,
      "loss": 0.0823,
      "step": 1350
    },
    {
      "epoch": 12.168141592920353,
      "grad_norm": 2.4481990337371826,
      "learning_rate": 8.062222222222222e-06,
      "loss": 0.0634,
      "step": 1375
    },
    {
      "epoch": 12.389380530973451,
      "grad_norm": 3.2933430671691895,
      "learning_rate": 8.006666666666667e-06,
      "loss": 0.0631,
      "step": 1400
    },
    {
      "epoch": 12.610619469026549,
      "grad_norm": 5.116746425628662,
      "learning_rate": 7.951111111111111e-06,
      "loss": 0.0641,
      "step": 1425
    },
    {
      "epoch": 12.831858407079647,
      "grad_norm": 4.75418758392334,
      "learning_rate": 7.895555555555557e-06,
      "loss": 0.0562,
      "step": 1450
    },
    {
      "epoch": 13.053097345132743,
      "grad_norm": 2.088742733001709,
      "learning_rate": 7.840000000000001e-06,
      "loss": 0.0621,
      "step": 1475
    },
    {
      "epoch": 13.274336283185841,
      "grad_norm": 1.6951649188995361,
      "learning_rate": 7.784444444444445e-06,
      "loss": 0.0415,
      "step": 1500
    },
    {
      "epoch": 13.495575221238939,
      "grad_norm": 2.0404789447784424,
      "learning_rate": 7.72888888888889e-06,
      "loss": 0.0368,
      "step": 1525
    },
    {
      "epoch": 13.716814159292035,
      "grad_norm": 1.9101096391677856,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.045,
      "step": 1550
    },
    {
      "epoch": 13.938053097345133,
      "grad_norm": 3.6267764568328857,
      "learning_rate": 7.617777777777778e-06,
      "loss": 0.0478,
      "step": 1575
    },
    {
      "epoch": 14.15929203539823,
      "grad_norm": 1.3897974491119385,
      "learning_rate": 7.562222222222223e-06,
      "loss": 0.0325,
      "step": 1600
    },
    {
      "epoch": 14.380530973451327,
      "grad_norm": 1.6585173606872559,
      "learning_rate": 7.506666666666668e-06,
      "loss": 0.0302,
      "step": 1625
    },
    {
      "epoch": 14.601769911504425,
      "grad_norm": 2.465259552001953,
      "learning_rate": 7.451111111111111e-06,
      "loss": 0.0323,
      "step": 1650
    },
    {
      "epoch": 14.823008849557523,
      "grad_norm": 1.5263018608093262,
      "learning_rate": 7.395555555555556e-06,
      "loss": 0.0291,
      "step": 1675
    },
    {
      "epoch": 15.044247787610619,
      "grad_norm": 5.142396926879883,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.0311,
      "step": 1700
    },
    {
      "epoch": 15.265486725663717,
      "grad_norm": 1.1755945682525635,
      "learning_rate": 7.284444444444445e-06,
      "loss": 0.0196,
      "step": 1725
    },
    {
      "epoch": 15.486725663716815,
      "grad_norm": 1.1286413669586182,
      "learning_rate": 7.22888888888889e-06,
      "loss": 0.0198,
      "step": 1750
    },
    {
      "epoch": 15.70796460176991,
      "grad_norm": 1.24678635597229,
      "learning_rate": 7.173333333333335e-06,
      "loss": 0.0215,
      "step": 1775
    },
    {
      "epoch": 15.929203539823009,
      "grad_norm": 1.5977578163146973,
      "learning_rate": 7.117777777777778e-06,
      "loss": 0.0274,
      "step": 1800
    },
    {
      "epoch": 16.150442477876105,
      "grad_norm": 0.6388368010520935,
      "learning_rate": 7.062222222222223e-06,
      "loss": 0.0177,
      "step": 1825
    },
    {
      "epoch": 16.371681415929203,
      "grad_norm": 0.7328804731369019,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.0182,
      "step": 1850
    },
    {
      "epoch": 16.5929203539823,
      "grad_norm": 0.8424196243286133,
      "learning_rate": 6.951111111111112e-06,
      "loss": 0.0163,
      "step": 1875
    },
    {
      "epoch": 16.8141592920354,
      "grad_norm": 0.9495406150817871,
      "learning_rate": 6.8955555555555565e-06,
      "loss": 0.0151,
      "step": 1900
    },
    {
      "epoch": 17.035398230088497,
      "grad_norm": 0.9939972162246704,
      "learning_rate": 6.8400000000000014e-06,
      "loss": 0.0151,
      "step": 1925
    },
    {
      "epoch": 17.256637168141594,
      "grad_norm": 3.349682331085205,
      "learning_rate": 6.784444444444445e-06,
      "loss": 0.0115,
      "step": 1950
    },
    {
      "epoch": 17.47787610619469,
      "grad_norm": 1.1144520044326782,
      "learning_rate": 6.7288888888888895e-06,
      "loss": 0.011,
      "step": 1975
    },
    {
      "epoch": 17.699115044247787,
      "grad_norm": 0.6789080500602722,
      "learning_rate": 6.6733333333333335e-06,
      "loss": 0.0102,
      "step": 2000
    },
    {
      "epoch": 17.699115044247787,
      "eval_loss": 0.9609427452087402,
      "eval_runtime": 75.6903,
      "eval_samples_per_second": 3.501,
      "eval_steps_per_second": 0.449,
      "eval_wer": 54.28321678321678,
      "step": 2000
    },
    {
      "epoch": 17.920353982300885,
      "grad_norm": 0.7190982103347778,
      "learning_rate": 6.617777777777778e-06,
      "loss": 0.0127,
      "step": 2025
    },
    {
      "epoch": 18.141592920353983,
      "grad_norm": 0.4549352824687958,
      "learning_rate": 6.562222222222223e-06,
      "loss": 0.0104,
      "step": 2050
    },
    {
      "epoch": 18.36283185840708,
      "grad_norm": 0.4192890226840973,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0092,
      "step": 2075
    },
    {
      "epoch": 18.58407079646018,
      "grad_norm": 0.6292315721511841,
      "learning_rate": 6.451111111111111e-06,
      "loss": 0.0081,
      "step": 2100
    },
    {
      "epoch": 18.805309734513273,
      "grad_norm": 0.4391574561595917,
      "learning_rate": 6.395555555555556e-06,
      "loss": 0.0083,
      "step": 2125
    },
    {
      "epoch": 19.02654867256637,
      "grad_norm": 1.8540560007095337,
      "learning_rate": 6.34e-06,
      "loss": 0.0086,
      "step": 2150
    },
    {
      "epoch": 19.24778761061947,
      "grad_norm": 0.3467632830142975,
      "learning_rate": 6.284444444444445e-06,
      "loss": 0.0061,
      "step": 2175
    },
    {
      "epoch": 19.469026548672566,
      "grad_norm": 0.5162904262542725,
      "learning_rate": 6.22888888888889e-06,
      "loss": 0.006,
      "step": 2200
    },
    {
      "epoch": 19.690265486725664,
      "grad_norm": 0.34368789196014404,
      "learning_rate": 6.173333333333333e-06,
      "loss": 0.007,
      "step": 2225
    },
    {
      "epoch": 19.911504424778762,
      "grad_norm": 0.3621026575565338,
      "learning_rate": 6.117777777777778e-06,
      "loss": 0.0072,
      "step": 2250
    },
    {
      "epoch": 20.13274336283186,
      "grad_norm": 0.4760856330394745,
      "learning_rate": 6.062222222222223e-06,
      "loss": 0.0058,
      "step": 2275
    },
    {
      "epoch": 20.353982300884955,
      "grad_norm": 0.301374614238739,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.0052,
      "step": 2300
    },
    {
      "epoch": 20.575221238938052,
      "grad_norm": 5.954403400421143,
      "learning_rate": 5.951111111111112e-06,
      "loss": 0.0056,
      "step": 2325
    },
    {
      "epoch": 20.79646017699115,
      "grad_norm": 0.3289724588394165,
      "learning_rate": 5.895555555555557e-06,
      "loss": 0.0054,
      "step": 2350
    },
    {
      "epoch": 21.01769911504425,
      "grad_norm": 0.2755318284034729,
      "learning_rate": 5.84e-06,
      "loss": 0.005,
      "step": 2375
    },
    {
      "epoch": 21.238938053097346,
      "grad_norm": 0.2095494419336319,
      "learning_rate": 5.784444444444445e-06,
      "loss": 0.0047,
      "step": 2400
    },
    {
      "epoch": 21.460176991150444,
      "grad_norm": 0.2321719378232956,
      "learning_rate": 5.72888888888889e-06,
      "loss": 0.0047,
      "step": 2425
    },
    {
      "epoch": 21.68141592920354,
      "grad_norm": 0.2191598266363144,
      "learning_rate": 5.673333333333334e-06,
      "loss": 0.0046,
      "step": 2450
    },
    {
      "epoch": 21.902654867256636,
      "grad_norm": 8.523866653442383,
      "learning_rate": 5.617777777777779e-06,
      "loss": 0.0055,
      "step": 2475
    },
    {
      "epoch": 22.123893805309734,
      "grad_norm": 1.0972295999526978,
      "learning_rate": 5.562222222222222e-06,
      "loss": 0.0043,
      "step": 2500
    },
    {
      "epoch": 22.345132743362832,
      "grad_norm": 0.2148185521364212,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0047,
      "step": 2525
    },
    {
      "epoch": 22.56637168141593,
      "grad_norm": 0.19287139177322388,
      "learning_rate": 5.451111111111112e-06,
      "loss": 0.0039,
      "step": 2550
    },
    {
      "epoch": 22.787610619469028,
      "grad_norm": 0.3723675310611725,
      "learning_rate": 5.3955555555555565e-06,
      "loss": 0.0038,
      "step": 2575
    },
    {
      "epoch": 23.008849557522122,
      "grad_norm": 0.16049274802207947,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.0036,
      "step": 2600
    },
    {
      "epoch": 23.23008849557522,
      "grad_norm": 0.17020337283611298,
      "learning_rate": 5.2844444444444454e-06,
      "loss": 0.0037,
      "step": 2625
    },
    {
      "epoch": 23.451327433628318,
      "grad_norm": 0.1560908555984497,
      "learning_rate": 5.228888888888889e-06,
      "loss": 0.0035,
      "step": 2650
    },
    {
      "epoch": 23.672566371681416,
      "grad_norm": 0.20238395035266876,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0047,
      "step": 2675
    },
    {
      "epoch": 23.893805309734514,
      "grad_norm": 0.16711880266666412,
      "learning_rate": 5.117777777777778e-06,
      "loss": 0.0035,
      "step": 2700
    },
    {
      "epoch": 24.115044247787612,
      "grad_norm": 4.51506233215332,
      "learning_rate": 5.062222222222222e-06,
      "loss": 0.0033,
      "step": 2725
    },
    {
      "epoch": 24.336283185840706,
      "grad_norm": 0.18119944632053375,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.003,
      "step": 2750
    },
    {
      "epoch": 24.557522123893804,
      "grad_norm": 0.15523011982440948,
      "learning_rate": 4.951111111111111e-06,
      "loss": 0.0029,
      "step": 2775
    },
    {
      "epoch": 24.778761061946902,
      "grad_norm": 0.16000385582447052,
      "learning_rate": 4.895555555555556e-06,
      "loss": 0.0035,
      "step": 2800
    },
    {
      "epoch": 25.0,
      "grad_norm": 2.6194007396698,
      "learning_rate": 4.84e-06,
      "loss": 0.0033,
      "step": 2825
    },
    {
      "epoch": 25.221238938053098,
      "grad_norm": 0.14104199409484863,
      "learning_rate": 4.784444444444445e-06,
      "loss": 0.0034,
      "step": 2850
    },
    {
      "epoch": 25.442477876106196,
      "grad_norm": 0.1463729441165924,
      "learning_rate": 4.728888888888889e-06,
      "loss": 0.0031,
      "step": 2875
    },
    {
      "epoch": 25.663716814159294,
      "grad_norm": 0.11884653568267822,
      "learning_rate": 4.673333333333333e-06,
      "loss": 0.0027,
      "step": 2900
    },
    {
      "epoch": 25.884955752212388,
      "grad_norm": 0.14391283690929413,
      "learning_rate": 4.617777777777778e-06,
      "loss": 0.0026,
      "step": 2925
    },
    {
      "epoch": 26.106194690265486,
      "grad_norm": 0.11210306733846664,
      "learning_rate": 4.562222222222222e-06,
      "loss": 0.0025,
      "step": 2950
    },
    {
      "epoch": 26.327433628318584,
      "grad_norm": 0.14337527751922607,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0024,
      "step": 2975
    },
    {
      "epoch": 26.548672566371682,
      "grad_norm": 0.11500510573387146,
      "learning_rate": 4.451111111111112e-06,
      "loss": 0.0024,
      "step": 3000
    },
    {
      "epoch": 26.548672566371682,
      "eval_loss": 1.0443298816680908,
      "eval_runtime": 75.6655,
      "eval_samples_per_second": 3.502,
      "eval_steps_per_second": 0.449,
      "eval_wer": 54.28321678321678,
      "step": 3000
    },
    {
      "epoch": 26.76991150442478,
      "grad_norm": 0.13586698472499847,
      "learning_rate": 4.395555555555556e-06,
      "loss": 0.0024,
      "step": 3025
    },
    {
      "epoch": 26.991150442477878,
      "grad_norm": 0.11220082640647888,
      "learning_rate": 4.34e-06,
      "loss": 0.0024,
      "step": 3050
    },
    {
      "epoch": 27.212389380530972,
      "grad_norm": 0.09876510500907898,
      "learning_rate": 4.284444444444445e-06,
      "loss": 0.0022,
      "step": 3075
    },
    {
      "epoch": 27.43362831858407,
      "grad_norm": 0.12959891557693481,
      "learning_rate": 4.228888888888889e-06,
      "loss": 0.0022,
      "step": 3100
    },
    {
      "epoch": 27.654867256637168,
      "grad_norm": 0.13237859308719635,
      "learning_rate": 4.173333333333334e-06,
      "loss": 0.0022,
      "step": 3125
    },
    {
      "epoch": 27.876106194690266,
      "grad_norm": 0.10337714850902557,
      "learning_rate": 4.117777777777779e-06,
      "loss": 0.0023,
      "step": 3150
    },
    {
      "epoch": 28.097345132743364,
      "grad_norm": 0.10730256140232086,
      "learning_rate": 4.062222222222223e-06,
      "loss": 0.0021,
      "step": 3175
    },
    {
      "epoch": 28.31858407079646,
      "grad_norm": 0.09900732338428497,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.002,
      "step": 3200
    },
    {
      "epoch": 28.539823008849556,
      "grad_norm": 0.09218195080757141,
      "learning_rate": 3.951111111111112e-06,
      "loss": 0.002,
      "step": 3225
    },
    {
      "epoch": 28.761061946902654,
      "grad_norm": 0.12199623882770538,
      "learning_rate": 3.895555555555556e-06,
      "loss": 0.0021,
      "step": 3250
    },
    {
      "epoch": 28.98230088495575,
      "grad_norm": 0.10119417309761047,
      "learning_rate": 3.8400000000000005e-06,
      "loss": 0.002,
      "step": 3275
    },
    {
      "epoch": 29.20353982300885,
      "grad_norm": 0.08351318538188934,
      "learning_rate": 3.784444444444445e-06,
      "loss": 0.0018,
      "step": 3300
    },
    {
      "epoch": 29.424778761061948,
      "grad_norm": 0.09863772243261337,
      "learning_rate": 3.728888888888889e-06,
      "loss": 0.0019,
      "step": 3325
    },
    {
      "epoch": 29.646017699115045,
      "grad_norm": 0.09792592376470566,
      "learning_rate": 3.673333333333334e-06,
      "loss": 0.0019,
      "step": 3350
    },
    {
      "epoch": 29.86725663716814,
      "grad_norm": 0.10190178453922272,
      "learning_rate": 3.617777777777778e-06,
      "loss": 0.0019,
      "step": 3375
    },
    {
      "epoch": 30.088495575221238,
      "grad_norm": 0.09422311931848526,
      "learning_rate": 3.5622222222222224e-06,
      "loss": 0.0018,
      "step": 3400
    },
    {
      "epoch": 30.309734513274336,
      "grad_norm": 0.08409751206636429,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0018,
      "step": 3425
    },
    {
      "epoch": 30.530973451327434,
      "grad_norm": 0.0847754180431366,
      "learning_rate": 3.4511111111111113e-06,
      "loss": 0.0017,
      "step": 3450
    },
    {
      "epoch": 30.75221238938053,
      "grad_norm": 0.09780299663543701,
      "learning_rate": 3.3955555555555558e-06,
      "loss": 0.0018,
      "step": 3475
    },
    {
      "epoch": 30.97345132743363,
      "grad_norm": 0.08742529153823853,
      "learning_rate": 3.3400000000000006e-06,
      "loss": 0.0018,
      "step": 3500
    },
    {
      "epoch": 31.194690265486727,
      "grad_norm": 0.08645310997962952,
      "learning_rate": 3.2844444444444447e-06,
      "loss": 0.0017,
      "step": 3525
    },
    {
      "epoch": 31.41592920353982,
      "grad_norm": 0.08412289619445801,
      "learning_rate": 3.228888888888889e-06,
      "loss": 0.0017,
      "step": 3550
    },
    {
      "epoch": 31.63716814159292,
      "grad_norm": 0.09290935099124908,
      "learning_rate": 3.173333333333334e-06,
      "loss": 0.0017,
      "step": 3575
    },
    {
      "epoch": 31.858407079646017,
      "grad_norm": 0.08525004982948303,
      "learning_rate": 3.117777777777778e-06,
      "loss": 0.0017,
      "step": 3600
    },
    {
      "epoch": 32.07964601769911,
      "grad_norm": 0.0707017332315445,
      "learning_rate": 3.0622222222222225e-06,
      "loss": 0.0016,
      "step": 3625
    },
    {
      "epoch": 32.30088495575221,
      "grad_norm": 0.06781639158725739,
      "learning_rate": 3.0066666666666674e-06,
      "loss": 0.0016,
      "step": 3650
    },
    {
      "epoch": 32.52212389380531,
      "grad_norm": 0.08131064474582672,
      "learning_rate": 2.9511111111111114e-06,
      "loss": 0.0016,
      "step": 3675
    },
    {
      "epoch": 32.743362831858406,
      "grad_norm": 0.07680153101682663,
      "learning_rate": 2.895555555555556e-06,
      "loss": 0.0016,
      "step": 3700
    },
    {
      "epoch": 32.9646017699115,
      "grad_norm": 0.09204059094190598,
      "learning_rate": 2.84e-06,
      "loss": 0.0016,
      "step": 3725
    },
    {
      "epoch": 33.1858407079646,
      "grad_norm": 0.07204937189817429,
      "learning_rate": 2.784444444444445e-06,
      "loss": 0.0015,
      "step": 3750
    },
    {
      "epoch": 33.4070796460177,
      "grad_norm": 0.07051807641983032,
      "learning_rate": 2.7288888888888893e-06,
      "loss": 0.0015,
      "step": 3775
    },
    {
      "epoch": 33.6283185840708,
      "grad_norm": 0.0780523493885994,
      "learning_rate": 2.6733333333333333e-06,
      "loss": 0.0015,
      "step": 3800
    },
    {
      "epoch": 33.849557522123895,
      "grad_norm": 0.07153183966875076,
      "learning_rate": 2.617777777777778e-06,
      "loss": 0.0015,
      "step": 3825
    },
    {
      "epoch": 34.07079646017699,
      "grad_norm": 0.07550474256277084,
      "learning_rate": 2.5622222222222226e-06,
      "loss": 0.0015,
      "step": 3850
    },
    {
      "epoch": 34.29203539823009,
      "grad_norm": 0.07227027416229248,
      "learning_rate": 2.5066666666666667e-06,
      "loss": 0.0014,
      "step": 3875
    },
    {
      "epoch": 34.51327433628319,
      "grad_norm": 0.07236452400684357,
      "learning_rate": 2.451111111111111e-06,
      "loss": 0.0014,
      "step": 3900
    },
    {
      "epoch": 34.73451327433628,
      "grad_norm": 0.08092208951711655,
      "learning_rate": 2.3955555555555556e-06,
      "loss": 0.0014,
      "step": 3925
    },
    {
      "epoch": 34.95575221238938,
      "grad_norm": 0.08697052299976349,
      "learning_rate": 2.3400000000000005e-06,
      "loss": 0.0014,
      "step": 3950
    },
    {
      "epoch": 35.176991150442475,
      "grad_norm": 0.07541479915380478,
      "learning_rate": 2.2844444444444445e-06,
      "loss": 0.0014,
      "step": 3975
    },
    {
      "epoch": 35.39823008849557,
      "grad_norm": 0.06794344633817673,
      "learning_rate": 2.228888888888889e-06,
      "loss": 0.0013,
      "step": 4000
    },
    {
      "epoch": 35.39823008849557,
      "eval_loss": 1.0885708332061768,
      "eval_runtime": 74.2159,
      "eval_samples_per_second": 3.571,
      "eval_steps_per_second": 0.458,
      "eval_wer": 54.05011655011654,
      "step": 4000
    }
  ],
  "logging_steps": 25,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 45,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5687153598464e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
